{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T05:37:16.534913Z",
     "start_time": "2025-10-24T05:37:11.579541Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- (11일차 추가) 데이터 분리 도구 ---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. (10일차 수정) Dataset 클래스 ---\n",
    "# csv_path 대신 'DataFrame'과 'scaler'를 직접 받도록 수정\n",
    "class JeonseDataset(Dataset):\n",
    "\n",
    "    # __init__을 대폭 수정\n",
    "    def __init__(self, dataframe, scaler, is_train=True):\n",
    "        self.scaler = scaler\n",
    "        feature_cols = dataframe.columns.drop('risk_label')\n",
    "\n",
    "        # is_train 플래그 대신, fit_transform은 이 클래스 밖에서 수행\n",
    "        # __init__에서는 transform만 수행\n",
    "        self.features = self.scaler.transform(dataframe[feature_cols])\n",
    "        self.labels = dataframe['risk_label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        feature_tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return feature_tensor, label_tensor.view(1)\n",
    "\n",
    "# --- 2. (5일차) 모델(MLP) 클래스 (동일) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x); out = self.relu(out)\n",
    "        out = self.layer2(out); out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# --- 3. (11일차 핵심) 데이터 준비 (분리 및 스케일링) ---\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 3-1. 원본 데이터 로드\n",
    "csv_file_path = 'dummy_data.csv' # 8일차에 생성한 파일\n",
    "full_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# 3-2. 훈련 / 검증 데이터 분리 (예: 80% 훈련, 20% 검증)\n",
    "train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"총 데이터: {len(full_df)}개\")\n",
    "print(f\"훈련 데이터: {len(train_df)}개, 검증 데이터: {len(val_df)}개\")\n",
    "\n",
    "# 3-3. 스케일러(Scaler) 준비 (데이터 누수 방지!)\n",
    "# (중요!) Scaler는 'train_df'로만 'fit' 해야 합니다.\n",
    "feature_cols = train_df.columns.drop('risk_label')\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[feature_cols]) # 훈련셋으로만 평균, 표준편차 계산\n",
    "\n",
    "# 3-4. Dataset 및 DataLoader 준비\n",
    "# 훈련셋: 훈련용 scaler를 전달\n",
    "train_dataset = JeonseDataset(train_df, scaler)\n",
    "# 검증셋: '똑같은' 훈련용 scaler를 전달\n",
    "val_dataset = JeonseDataset(val_df, scaler)\n",
    "\n",
    "# Hyperparameters (동일)\n",
    "input_dim = 3; hidden_dim = 8; output_dim = 1\n",
    "learning_rate = 0.001; batch_size = 16; num_epochs = 50\n",
    "\n",
    "# DataLoader 준비\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# (중요!) 검증용(val_loader)은 섞을(shuffle) 필요가 없습니다.\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# (모델, 손실함수, 옵티마이저 - 동일)\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"--- 훈련/검증 분리 후 학습 시작 ---\")\n",
    "\n",
    "# --- 4. (11일차 핵심) 훈련/검증 루프 분리 ---\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # === 훈련(Training) 루프 ===\n",
    "    model.train() # 훈련 모드\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    for features_batch, labels_batch in train_loader:\n",
    "        features_batch = features_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        prediction = model(features_batch)\n",
    "        loss = loss_fn(prediction, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # 훈련 O\n",
    "        optimizer.step()  # 훈련 O\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (prediction.round() == labels_batch).sum().item()\n",
    "\n",
    "    # === 검증(Validation) 루프 ===\n",
    "    model.eval() # 평가 모드\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad(): # (핵심!) 기울기 계산 비활성화\n",
    "        for features_batch, labels_batch in val_loader:\n",
    "            features_batch = features_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            prediction = model(features_batch)\n",
    "            loss = loss_fn(prediction, labels_batch)\n",
    "\n",
    "            # .backward() / .step() 없음!\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (prediction.round() == labels_batch).sum().item()\n",
    "\n",
    "    # --- Epoch 종료: 훈련/검증 결과 출력 ---\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_train_acc = train_correct / len(train_dataset)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_val_acc = val_correct / len(val_dataset)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1:3d}/{num_epochs}]\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f} | Train Acc: {avg_train_acc * 100:.2f}%\")\n",
    "        print(f\"  Valid Loss: {avg_val_loss:.4f} | Valid Acc: {avg_val_acc * 100:.2f}%\")\n",
    "\n",
    "print(\"--- 학습 완료 ---\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 데이터: 100개\n",
      "훈련 데이터: 80개, 검증 데이터: 20개\n",
      "--- 훈련/검증 분리 후 학습 시작 ---\n",
      "Epoch [  5/50]\n",
      "  Train Loss: 0.6998 | Train Acc: 55.00%\n",
      "  Valid Loss: 0.7013 | Valid Acc: 50.00%\n",
      "Epoch [ 10/50]\n",
      "  Train Loss: 0.6964 | Train Acc: 55.00%\n",
      "  Valid Loss: 0.6887 | Valid Acc: 55.00%\n",
      "Epoch [ 15/50]\n",
      "  Train Loss: 0.6929 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6781 | Valid Acc: 55.00%\n",
      "Epoch [ 20/50]\n",
      "  Train Loss: 0.6899 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6678 | Valid Acc: 55.00%\n",
      "Epoch [ 25/50]\n",
      "  Train Loss: 0.6871 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6586 | Valid Acc: 55.00%\n",
      "Epoch [ 30/50]\n",
      "  Train Loss: 0.6845 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6503 | Valid Acc: 55.00%\n",
      "Epoch [ 35/50]\n",
      "  Train Loss: 0.6821 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6418 | Valid Acc: 55.00%\n",
      "Epoch [ 40/50]\n",
      "  Train Loss: 0.6798 | Train Acc: 57.50%\n",
      "  Valid Loss: 0.6352 | Valid Acc: 55.00%\n",
      "Epoch [ 45/50]\n",
      "  Train Loss: 0.6778 | Train Acc: 57.50%\n",
      "  Valid Loss: 0.6283 | Valid Acc: 55.00%\n",
      "Epoch [ 50/50]\n",
      "  Train Loss: 0.6759 | Train Acc: 56.25%\n",
      "  Valid Loss: 0.6224 | Valid Acc: 55.00%\n",
      "--- 학습 완료 ---\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
