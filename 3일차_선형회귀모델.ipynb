{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-19T23:50:10.469518Z",
     "start_time": "2025-10-19T23:50:05.328911Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- 1. 데이터 준비 (1일차 복습) ---\n",
    "# 공부 시간을 0, 1, 2, 3 시간 (입력, x)\n",
    "# x_train은 [4, 1] 모양의 텐서가 되어야 합니다. (데이터 4개, feature 1개)\n",
    "x_train = torch.tensor([[0.0], [1.0], [2.0], [3.0]], dtype=torch.float32)\n",
    "\n",
    "# 그에 따른 성적을 1, 3, 5, 7 점 (정답, y)\n",
    "# y_train도 [4, 1] 모양\n",
    "y_train = torch.tensor([[1.0], [3.0], [5.0], [7.0]], dtype=torch.float32)\n",
    "\n",
    "# --- 2. 모델, 손실함수, 옵티마이저 정의 (3일차 핵심) ---\n",
    "\n",
    "# 모델 정의: nn.Linear(입력 feature 개수, 출력 feature 개수)\n",
    "# 입력(공부 시간) 1개, 출력(성적) 1개\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# 손실 함수 정의: 평균 제곱 오차 (MSE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 옵티마이저 정의: SGD\n",
    "# model.parameters()를 전달 -> 이 모델의 W, b를 학습 대상으로 지정\n",
    "# lr=0.01 은 '학습률(learning rate)'로, 얼마자 '조금씩' 이동할지 정하는 값\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"--- 학습 시작 전 ---\")\n",
    "# model.parameters()는 W와 b를 담고 있습니다.\n",
    "# list()로 감싸서 내부 값을 확인해봅니다.\n",
    "print(f\"초기 W: {list(model.parameters())[0].item():.3f}, 초기 b: {list(model.parameters())[1].item():.3f}\")\n",
    "\n",
    "\n",
    "# --- 3. 학습 루프 (Training Loop) 실행\n",
    "# 모델을 1000번 반복 학습시킵니다.\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs+1):\n",
    "    # 1. 순전파 (Forward Pass): 모델로 예측값 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # 2. 손실 계산: 예측값과 실제 정답 비교\n",
    "    loss=loss_fn(prediction, y_train)\n",
    "\n",
    "    # 3. (중요!) 기울기 초기화\n",
    "    # .backward()는 기울기를 '누적'시키므로, 매번 새로운 기울기를 계산하기 전에 초기화\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. 역전파 (Backward Pass): 손실에 대한 기울기 계산 (2일차 복습)\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. 파라미터 업데이트: 옵티마이저가 W, b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}/{epochs} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"\\n--- 학습 완료 후 ---\")\n",
    "# 학습된 W (기울기)와 b (절편) 확인\n",
    "trained_W = list(model.parameters())[0].item()\n",
    "trained_b = list(model.parameters())[1].item()\n",
    "print(f\"학습된 W: {trained_W:.3f}, 학습된 b: {trained_b:.3f}\")\n",
    "\n",
    "# --- 4. 학습된 모델로 예측해보기 ---\n",
    "test_x = torch.tensor([[4.0]]) # 4시간 공부하면?\n",
    "predicted_y=model(test_x)\n",
    "print(f\"\\n4시간 공부했을 때 예상 성적: {predicted_y.item():.3f} 점\")\n",
    "# (정답 W=2, b=1 이었다면 2*4+1=9.0에 가까운 값이 나옵니다.)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 학습 시작 전 ---\n",
      "초기 W: -0.050, 초기 b: 0.292\n",
      "Epoch    0/1000 | Loss: 19.5722\n",
      "Epoch  100/1000 | Loss: 0.0047\n",
      "Epoch  200/1000 | Loss: 0.0014\n",
      "Epoch  300/1000 | Loss: 0.0004\n",
      "Epoch  400/1000 | Loss: 0.0001\n",
      "Epoch  500/1000 | Loss: 0.0000\n",
      "Epoch  600/1000 | Loss: 0.0000\n",
      "Epoch  700/1000 | Loss: 0.0000\n",
      "Epoch  800/1000 | Loss: 0.0000\n",
      "Epoch  900/1000 | Loss: 0.0000\n",
      "Epoch 1000/1000 | Loss: 0.0000\n",
      "\n",
      "--- 학습 완료 후 ---\n",
      "학습된 W: 2.000, 학습된 b: 1.001\n",
      "\n",
      "4시간 공부했을 때 예상 성적: 9.000 점\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
