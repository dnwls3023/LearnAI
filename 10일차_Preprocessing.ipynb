{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-24T05:10:56.768927Z",
     "start_time": "2025-10-24T05:10:51.408448Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- (10일차 추가) StandardScaler 임포트 ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- 1. (8일차) Custom Dataset 클래스 (10일차 수정) ---\n",
    "class JeonseDataset(Dataset):\n",
    "\n",
    "    # __init__이 csv_path 외에 'scaler' 객체와 'is_train' 플래그를 받도록 수정\n",
    "    def __init__(self, csv_path, scaler=None, is_train=True):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # 'risk_label'을 제외한 모든 열을 features로 간주\n",
    "        feature_cols = df.columns.drop('risk_label')\n",
    "\n",
    "        # --- (10일차 핵심) 전처리 로직 ---\n",
    "        if is_train:\n",
    "            # 훈련용 데이터라면, 새로운 Scaler를 생성하고 fit/transform\n",
    "            self.scaler = StandardScaler()\n",
    "            self.features = self.scaler.fit_transform(df[feature_cols])\n",
    "        else:\n",
    "            # 테스트용 데이터라면, 훈련 때 사용한 scaler를 그대로 받아 transform\n",
    "            if scaler is None:\n",
    "                raise ValueError(\"테스트 데이터셋은 반드시 훈련용 scaler를 받아야 합니다.\")\n",
    "            self.scaler = scaler\n",
    "            self.features = self.scaler.transform(df[feature_cols])\n",
    "        # ------------------------------------\n",
    "\n",
    "        self.labels = df['risk_label'].values\n",
    "        print(f\"{'훈련' if is_train else '테스트'} 데이터셋 로드 완료. Scaler 적용됨.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # self.features는 이미 NumPy 배열이자 스케일링이 완료된 상태\n",
    "        feature = self.features[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        feature_tensor = torch.tensor(feature, dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return feature_tensor, label_tensor.view(1)\n",
    "\n",
    "    # (10일차 추가) 나중에 테스트할 때 사용할 scaler를 반환하는 헬퍼 함수\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "# --- 2. (5일차) 모델(MLP) 클래스 (9일차와 동일) ---\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x); out = self.relu(out)\n",
    "        out = self.layer2(out); out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# --- 3. (10일차 수정) 학습 준비 ---\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# (Hyperparameters - 9일차와 동일)\n",
    "input_dim = 3; hidden_dim = 8; output_dim = 1\n",
    "learning_rate = 0.001; batch_size = 16; num_epochs = 50\n",
    "\n",
    "# (데이터 준비 - 10일차 수정)\n",
    "csv_file_path = 'dummy_data.csv'\n",
    "# is_train=True로 설정하여 Dataset 내부에서 Scaler를 fit 하도록 함\n",
    "train_dataset = JeonseDataset(csv_file_path, is_train=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# (모델, 손실함수, 옵티마이저 - 9일차와 동일)\n",
    "model = SimpleMLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"--- 스케일링 적용 후 학습 시작 ---\")\n",
    "\n",
    "# --- 4. (9일차와 동일) 중첩 학습 루프 ---\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "\n",
    "    for features_batch, labels_batch in train_data_loader:\n",
    "        features_batch = features_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "\n",
    "        prediction = model(features_batch)\n",
    "        loss = loss_fn(prediction, labels_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        predicted_labels = prediction.round()\n",
    "        correct = (predicted_labels == labels_batch).sum().item()\n",
    "        epoch_accuracy += correct\n",
    "\n",
    "    avg_loss = epoch_loss / len(train_data_loader)\n",
    "    avg_accuracy = epoch_accuracy / len(train_dataset)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1:3d}/{num_epochs}] | Loss: {avg_loss:.4f} | Accuracy: {avg_accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"--- 학습 완료 ---\")\n",
    "\n",
    "# --- 5. (10일차) 스케일링 적용 후 예측 테스트 ---\n",
    "# 훈련에 사용된 scaler를 가져옴\n",
    "fitted_scaler = train_dataset.get_scaler()\n",
    "\n",
    "# 9일차와 동일한 테스트 샘플\n",
    "test_data_raw = np.array([[0.9, 0.5, 10.0]]) # [전세가율, 근저당, 10년차]\n",
    "\n",
    "# (핵심!) 테스트 샘플에도 '반드시' scaler.transform()을 적용\n",
    "test_data_scaled = fitted_scaler.transform(test_data_raw)\n",
    "\n",
    "print(f\"\\n원본 테스트 데이터: {test_data_raw}\")\n",
    "print(f\"스케일링된 테스트 데이터: {test_data_scaled}\")\n",
    "\n",
    "# 스케일링된 데이터를 텐서로 변환하여 모델에 입력\n",
    "test_tensor = torch.tensor(test_data_scaled, dtype=torch.float32).to(device)\n",
    "risk_prob = model(test_tensor)\n",
    "print(f\"테스트 샘플의 위험 확률: {risk_prob.item() * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터셋 로드 완료. Scaler 적용됨.\n",
      "--- 스케일링 적용 후 학습 시작 ---\n",
      "Epoch [  5/50] | Loss: 0.6867 | Accuracy: 58.00%\n",
      "Epoch [ 10/50] | Loss: 0.6789 | Accuracy: 57.00%\n",
      "Epoch [ 15/50] | Loss: 0.6834 | Accuracy: 57.00%\n",
      "Epoch [ 20/50] | Loss: 0.6754 | Accuracy: 57.00%\n",
      "Epoch [ 25/50] | Loss: 0.6749 | Accuracy: 57.00%\n",
      "Epoch [ 30/50] | Loss: 0.6855 | Accuracy: 57.00%\n",
      "Epoch [ 35/50] | Loss: 0.6680 | Accuracy: 57.00%\n",
      "Epoch [ 40/50] | Loss: 0.6641 | Accuracy: 57.00%\n",
      "Epoch [ 45/50] | Loss: 0.6627 | Accuracy: 55.00%\n",
      "Epoch [ 50/50] | Loss: 0.6711 | Accuracy: 56.00%\n",
      "--- 학습 완료 ---\n",
      "\n",
      "원본 테스트 데이터: [[ 0.9  0.5 10. ]]\n",
      "스케일링된 테스트 데이터: [[ 0.35400675  0.46726213 -0.61904123]]\n",
      "테스트 샘플의 위험 확률: 58.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dnwls\\PycharmProjects\\JupyterProject\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
